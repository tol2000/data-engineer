{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прием и обработка твитов микробатчем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StringType, IntegerType, TimestampType}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.toree.kernel.api\n",
    "import java.util.Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current spark version is 2.4.4\n"
     ]
    }
   ],
   "source": [
    "println(s\"Current spark version is ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelPath = /home/jovyan/models/spark-ml-model\n",
       "model = pipeline_e3869da6fdc9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_e3869da6fdc9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelPath = \"/home/jovyan/models/spark-ml-model\"\n",
    "val model = PipelineModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем схему и инициируем потоковый датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputStreamPath = /home/jovyan/work/events-stream\n",
       "getProbability = UserDefinedFunction(<function2>,DoubleType,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7, IntegerType)))\n",
       "dataSchema = StructType(StructField(tweet,StringType,true), StructField(hiddentargetclue,IntegerType,true), StructField(timestamp,TimestampType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(tweet,StringType,true), StructField(hiddentargetclue,IntegerType,true), StructField(timestamp,TimestampType,true))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputStreamPath = \"/home/jovyan/work/events-stream\"\n",
    "//val outputStreamPath = \"/home/jovyan/work/events-stream-out\"\n",
    "\n",
    "// Определяем udf для получения probability по 0 и 1\n",
    "val getProbability =\n",
    "    udf(\n",
    "        (prediction: org.apache.spark.ml.linalg.Vector, pos: Integer) =>\n",
    "        {\n",
    "            prediction(pos)\n",
    "        }\n",
    "    )\n",
    "\n",
    "val dataSchema = new StructType()\n",
    "    .add(\"tweet\", StringType)\n",
    "    .add(\"hiddentargetclue\", IntegerType)\n",
    "    .add(\"timestamp\", TimestampType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputDF = [timestamp: struct<start: timestamp, end: timestamp>, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: struct<start: timestamp, end: timestamp>, count: bigint]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "                    .select(\n",
    "                        $\"arrived_key\",\n",
    "                        $\"tweet\",\n",
    "                        // $\"hiddentargetclue\",\n",
    "                        (getProbability($\"probability\",lit(0))).alias(\"Negative Probability\")\n",
    "                    )\n",
    "*/\n",
    "\n",
    "/*\n",
    "                model.transform(batchDF)\n",
    "                    .select(\n",
    "                        $\"timestamp\",\n",
    "                        $\"tweets\"\n",
    "                    )\n",
    "*/\n",
    "\n",
    "val inputDF = spark\n",
    "    .readStream\n",
    "    .schema(dataSchema)\n",
    "//    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .json(inputStreamPath)\n",
    "    .withWatermark(\"timestamp\", \"15 seconds\")\n",
    "    .groupBy(\n",
    "        window($\"timestamp\", \"10 seconds\", \"5 seconds\").alias(\"timestamp\")\n",
    "    )\n",
    "    .count()\n",
    "    //.agg(count(\"*\")).alias(\"tweets\")\n",
    "\n",
    "//val wStream = inputDF.writeStream\n",
    "//    .option(\"checkpointLocation\", outputStreamPath)\n",
    "//.start(outputStreamPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Микробатч приема твитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fRuns = 0\n",
       "fTweets = 0\n",
       "stream = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5982365e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5982365e"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|timestamp|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|           timestamp|count|\n",
      "+--------------------+-----+\n",
      "|[2020-01-27 23:48...|   32|\n",
      "|[2020-01-27 23:56...|   17|\n",
      "|[2020-01-27 23:58...|   19|\n",
      "|[2020-01-27 23:49...|   38|\n",
      "|[2020-01-27 23:53...|   27|\n",
      "|[2020-01-27 23:43...|   32|\n",
      "|[2020-01-27 23:47...|   39|\n",
      "|[2020-01-27 23:54...|   17|\n",
      "|[2020-01-27 23:57...|    9|\n",
      "|[2020-01-27 23:37...|   37|\n",
      "|[2020-01-27 23:35...|   28|\n",
      "|[2020-01-27 23:50...|   16|\n",
      "|[2020-01-28 00:00...|   36|\n",
      "|[2020-01-28 00:00...|   20|\n",
      "|[2020-01-28 00:03...|   16|\n",
      "|[2020-01-27 23:46...|   21|\n",
      "|[2020-01-28 00:04...|   20|\n",
      "|[2020-01-28 00:06...|   13|\n",
      "|[2020-01-27 23:50...|   22|\n",
      "|[2020-01-27 23:34...|   21|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Job aborted due to stage failure: Task 6 in stage 799.0 failed 1 times, most recent failure: Lost task 6.0 in stage 799.0 (TID 21848, localhost, executor driver): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-133086cc-3873-494a-b513-1fdd6ff1b811/state/0/11/2.delta of HDFSStateStoreProvider[id = (op=0,part=11),dir = file:/tmp/temporary-133086cc-3873-494a-b513-1fdd6ff1b811/state/0/11]: file:/tmp/temporary-133086cc-3873-494a-b513-1fdd6ff1b811/state/0/11/2.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$6$$anonfun$apply$1.apply$mcVJ$sp(HDFSBackedStateStoreProvider.scala:384)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$6$$anonfun$apply$1.apply(HDFSBackedStateStoreProvider.scala:383)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$6$$anonfun$apply$1.apply(HDFSBackedStateStoreProvider.scala:383)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:73)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$6.apply(HDFSBackedStateStoreProvider.scala:383)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$6.apply(HDFSBackedStateStoreProvider.scala:356)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:535)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:356)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:204)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:371)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:88)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.FileNotFoundException: file:/tmp/temporary-133086cc-3873-494a-b513-1fdd6ff1b811/state/0/11/2.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:183)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:628)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:205)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:322)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:424)\n",
      "\t... 39 more\n",
      "\n",
      "Driver stacktrace:\n",
      "Job 465 cancelled part of cancelled job group 1fef4090-e3de-420f-9b1b-01afa4f3e299\n"
     ]
    }
   ],
   "source": [
    "var fRuns = 0\n",
    "var fTweets:Long = 0\n",
    "\n",
    "// Микробатч для вывода результата предсказания\n",
    "// Выводится вероятность негативного твита\n",
    "// В задании написано, что это последняя колонка, но она здесь вроде первая (в позиции 0)\n",
    "val stream = inputDF.writeStream.foreachBatch {\n",
    "    (batchDF: DataFrame, batchId: Long) => {\n",
    "        try {\n",
    "            fRuns += 1\n",
    "            //batchDF.foreach {\n",
    "            //    row:Row => {\n",
    "            //        fTweets += 1\n",
    "            //        //row.getAs(\"tweets\")\n",
    "            //    }\n",
    "            //}\n",
    "            batchDF.show()\n",
    "            //print(s\"${Calendar.getInstance().toInstant} - loaded $fTweets rows from the events stream $fRuns times\"+13.toChar)\n",
    "            // Применяем модель и получаем соотв. датасет с предсказаниями\n",
    "//            globDF = \n",
    "//                model.transform(batchDF)\n",
    "//                    .select(\n",
    "//                        $\"timestamp\",\n",
    "//                        $\"tweets\"\n",
    "//                    )\n",
    "            \n",
    "        } catch {\n",
    "            case e:Throwable => {\n",
    "                println(e.getMessage)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Останов чтения потока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
