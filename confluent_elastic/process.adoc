= ДЗ по занятию 20

== 1. Разверните и подготовьте окружение

Вам потребуется компьютер или виртуальная машина минимум с двумя ядрами и 8 гигабайтами памяти (оптимально - 4 ядра, 16 гигабайт памяти и 100 гигабайт на диске для данных). Вы можете использовать Mac или Linux.
Вам потребуются такие пакеты:

- docker, docker-compose
- git
- jq
- OpenJDK8 (от него нужен keytool)
- curl

Если вы используете виртуальную машину с Linux рекомендую поднять значение vm.max_map_count (sudo sysctl -w vm.max_map_count=262144 для изменения на живой машине, и/или добавить строчку vm.max_map_count=262144 в /etc/sysctl.conf).
Если вы используете Mac - поднимите объем оперативной памяти для Docker Desktop до 8 гигабайт.

Скопируйте себе репозиторий с окружением:

`git clone https://github.com/confluentinc/cp-demo && cd cp-demo`

В эту же директорию скопируйте скрипты set_elasticsearch_mapping_lang.sh и submit_elastic_sink_lang_config.sh которые приложены к заданию (не забудьте сделать их исполняемыми командой `chmod u+x`).

Запустите скрипт `./scripts/start.sh` (вы можете остановить окружение и удалить все собранные данные если запустите `./scripts/stop.sh`)

В зависимости от мощности вашей машины и ширины канала в интернет скрипт может занимать от нескольких минут до получаса (повторые запуски будут проходить быстрее, так как все образы уже будут скачаны).

Если все прошло успешно, то скрипт поприветствует вас сообщением:
```
******************************************************************
DONE! Connect to Confluent Control Center at http://localhost:9021
******************************************************************
```

Если у вас появились проблемы - обратитесь к преподавателю, либо воспользуйтесь подсказками [на сайте Confluent](https://docs.confluent.io/current/tutorials/cp-demo/docs/index.html#troubleshooting-the-demo).

Для работы, помимо текстового редактора, вам понадобятся 4 окна:
1. Браузер (лучше Chrome) с Confluent Control Center http://localhost:9021 (если вы запускаетесь где-то на виртуалке - используйте ее адрес)
2. Браузер c Kibana UI http://localhost:5601
3. Терминал с открытым KSQL CLI, для этого в директории ./cp-demo выполните команду `docker-compose exec ksql-cli ksql http://ksql-server:8088`
4. Терминал с shell в директории ./cp-demo для запуска скриптов и отладки

== 2. Создайте KSQL Stream WIKILANG

Посмотрите какие топики есть сейчас в системе, и на основе того, в котором вы видите максимальный объем данных создайте stream по имени WIKILANG который фильтрует правки только в разделах национальных языков, кроме английского (поле channel вида #ru.wikipedia), который сделали не боты.

Stream должен содержать следующие поля: createdat, channel, username, wikipage, diffurl

=== Решение.

wikipedia.parsed - стрим, где на текущий момент больше всего данных, а также самое большое потребление (consumed per second)

Поскольку есть условие при создании стрима, то мы будем использовать streaming ETL.
https://www.confluent.io/stream-processing-cookbook/ksql-recipes/data-filtering/

Смотрим какие есть стримы

```
ksql> show streams;

 Stream Name              | Kafka Topic                 | Format 
-----------------------------------------------------------------
 EN_WIKIPEDIA_GT_1_COUNTS | EN_WIKIPEDIA_GT_1_COUNTS    | AVRO   
 EN_WIKIPEDIA_GT_1_STREAM | EN_WIKIPEDIA_GT_1           | AVRO   
 KSQL_PROCESSING_LOG      | default_ksql_processing_log | JSON   
 WIKIPEDIA                | wikipedia.parsed            | AVRO   
 WIKIPEDIABOT             | WIKIPEDIABOT                | AVRO   
 WIKIPEDIANOBOT           | WIKIPEDIANOBOT              | AVRO   
-----------------------------------------------------------------
```

У нас уже есть стрим, основанный на wikipedia.parsed, это wikipedia.
Более того, есть стрим wikipedianobot, он-то нам и нужен.
Поэтому не будем изобретать велосипед, а создадим стрим на его основе.
Убедимся, что стрим тот, который нам подходит (смотрим на его запрос)

```
describe extended wikipedia;
describe extended wikipedianobot;
```

```
create stream wikilang as
  select createdat, channel, username, wikipage, diffurl
  from wikipedianobot
  where wikipedianobot.channel <> '#en.wikipedia';
```

== 3. Мониторинг WIKILANG

=== После 1-2 минут работы откройте Confluent Control Center и сравните пропускную способность топиков WIKILANG и WIKIPEDIANOBOT, какие числа вы видите?

==== Решение

Вижу такие цифры

```
wikilang 1092
wikipedianobot 2087
```

Отличие потоков сообщений от неботов на всех языках и на английском примерно в два раза.

=== В KSQL CLI получите текущую статистику вашего стрима

describe extended wikilang;

Приложите полный ответ на предыдущий запрос к ответу на задание.

```
ksql> describe extended wikilang;

Name                 : WIKILANG
Type                 : STREAM
Key field            : 
Key format           : STRING
Timestamp field      : Not set - using <ROWTIME>
Value format         : AVRO
Kafka topic          : WIKILANG (partitions: 2, replication: 2)

 Field     | Type                      
---------------------------------------
 ROWTIME   | BIGINT           (system) 
 ROWKEY    | VARCHAR(STRING)  (system) 
 CREATEDAT | BIGINT                    
 CHANNEL   | VARCHAR(STRING)           
 USERNAME  | VARCHAR(STRING)           
 WIKIPAGE  | VARCHAR(STRING)           
 DIFFURL   | VARCHAR(STRING)           
---------------------------------------

Queries that write from this STREAM
-----------------------------------
CSAS_WIKILANG_7 : CREATE STREAM WIKILANG WITH (KAFKA_TOPIC='WIKILANG', PARTITIONS=2, REPLICAS=2) AS SELECT
  WIKIPEDIANOBOT.CREATEDAT "CREATEDAT",
  WIKIPEDIANOBOT.CHANNEL "CHANNEL",
  WIKIPEDIANOBOT.USERNAME "USERNAME",
  WIKIPEDIANOBOT.WIKIPAGE "WIKIPAGE",
  WIKIPEDIANOBOT.DIFFURL "DIFFURL"
FROM WIKIPEDIANOBOT WIKIPEDIANOBOT
WHERE (WIKIPEDIANOBOT.CHANNEL <> '#en.wikipedia')
EMIT CHANGES;

For query topology and execution plan please run: EXPLAIN <QueryId>

Local runtime statistics
------------------------
messages-per-sec:      3.69   total-messages:      1375     last-message: 2020-03-18T18:04:26.009Z

(Statistics of the local KSQL server interaction with the Kafka topic WIKILANG)
```

- В KSQL CLI получите текущую статистику WIKIPEDIANOBOT: descrbie extended wikipedianobot;

Приложите раздел Local runtime statistics к ответу на задание.

```
Local runtime statistics
------------------------
consumer-messages-per-sec:      5.33 consumer-total-bytes:    654612 consumer-total-messages:      2874 messages-per-sec:      5.28   total-messages:      8154     last-message: 2020-03-18T18:06:20.532Z
```

Почему для wikipedianobot интерфейс показывает также consumer-* метрики?

У этого топика есть подписчики (консьюмеры).
У wikilang же их нет.

Правда, когда мы добавили консьюмера (эластик) к викилэнгу, то консьюмер-статистик все равно не появилось.
Возможно, консьюмер-метрики есть потому, что на основе стрима нобот создан другой стрим, а на основе викиланга - нет.
Хотя, это притянуто за уши, т.к. я мог и не создавать на основе нобота, а создать на основе чего-то другого, а в задании спрашивается, поччему именно у ноубота есть консьюмер-метрики.
В общем, надо покопать...)

== 4. Добавьте данные из стрима WIKILANG в ElasticSearch

- Добавьте mapping - запустите скрипт set_elasticsearch_mapping_lang.sh
- Добавьте Kafka Connect - запустите submit_elastic_sink_lang_config.sh
- Добавьте index-pattern - Kibana UI -> Management -> Index patterns -> Create Index Pattern -> Index name or pattern: wikilang -> кнопка Create

Используя полученные знания и документацию ответьте на вопросы:  

a) Опишите что делает каждая из этих операций?

- set_elasticsearch_mapping_lang.sh +
  Передаем через restful api эластика маппинг - структуру данных, которые будем читать.
- submit_elastic_sink_lang_config.sh +
  


б) Зачем Elasticsearch нужен mapping чтобы принять данные?

Кафка ничего не знает и не сообщает о структуре данных внутри себя.
Это забота подключающихся к ней приложений.
Мы определяем в эластике структуру данных, которые будем читать.


в) Что дает index-pattern?

== 5. Создайте отчет "Топ10 национальных разделов" на базе индекса wikilang

- Kibana UI -> Visualize -> + -> Data Table -> выберите индекс wikilang
- Select bucket type -> Split Rows, Aggregation -> Terms, Field -> CHANNEL.keyword, Size -> 10, нажмите кнопку Apply changes (выглядит как кнопка Play)
- Сохраните визуализацию под удобным для вас именем

Что вы увидели в отчете?

Аггрегацию количества записей по всем каналам нашего стрима wikilang.

[source]
----
#commons.wikimedia	941
#es.wikipedia	773
#de.wikipedia	379
#fr.wikipedia	313
#ru.wikipedia	253
#it.wikipedia	242
#uk.wikipedia	70
#en.wiktionary	61
#zh.wikipedia	51
#mediawiki.wikipedia	25
----

- Нажав маленьку круглую кнопку со стрелкой вверх под отчетом, вы сможете запросить не только таблицу, но и запрос на Query DSL которым он получен.

Приложите тело запроса к заданию.

[source, json]
----
{
  "query": {
    "bool": {
      "must": [
        {
          "match_all": {}
        },
        {
          "range": {
            "CREATEDAT": {
              "gte": 1584556932540,
              "lte": 1584557832540,
              "format": "epoch_millis"
            }
          }
        }
      ],
      "must_not": []
    }
  },
  "size": 0,
  "_source": {
    "excludes": []
  },
  "aggs": {
    "2": {
      "terms": {
        "field": "CHANNEL.keyword",
        "size": 10,
        "order": {
          "_count": "desc"
        }
      }
    }
  }
}
----